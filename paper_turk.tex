\documentclass[11pt]{article}

%\usepackage{amsmath, amssymb, fullpage, amsthm, array, algorithm2e,graphicx}
\usepackage{graphicx, amsmath}
\usepackage{url}
\RequirePackage{natbib}
\usepackage[colorlinks=true, citecolor=blue, linkcolor=blue]{hyperref}

\graphicspath{{images/}}

\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.3in}
\setlength{\textheight}{8.8in}


%\setlength{\evensidemargin}{0.25 in}
%\setlength{\textwidth}{6.2 in}           % allow for a little sloppiness (6.3)
%\setlength{\topmargin}{-.30 in}           % top margin to header (was -.35)
%\setlength{\textheight}{8.8 in} 


\title{Designing Turk Experiments for Visual Statistical Inference}
\author{Mahbubul Majumder, Heike Hofmann, Dianne Cook\\
        Department of Statistics, Iowa State University}


\begin{document}

\tableofcontents

\maketitle


\begin {abstract} 

Human observers are needed to evaluate lineups that are used to test the significance of findings using statistical graphics. One good option is to recruit people from online workplace like Amazon Mechanical Turk (MTurk). It provides features to create online task that allow people to evaluate lineups and get paid. MTurk is designed for simple and easy tasks. The technical design of the underlying experiment for lineup evaluation may be complex and the tools available to design this from MTurk is just too simple. In this paper we present the design of MTurk experiments for lineup evaluation, provide an alternative way to conduct the survey on lineups by developing a separate web application and getting turk worker do their job from that web site. The web site is now hosted on the Iowa State University public domain and has been in use for multiple experiments \citep{majumder:turk}. It turns out to be a very efficient way of getting lineups evaluated by online observers. Getting results from this web site is very convenient and secure.

{\bf Keywords: \sf statistical graphics, lineup, Amazon Mechanical Turk, visual inference, visualization, Crowdsourcing, Human Intelligence.}

\end {abstract}


\section{Introduction} 

There have been some advancements in visual statistical inference since it was first introduced by \cite{buja:2009} where they have proposed formal methods for testing the significance of findings using lineup protocols. In visual statistical inference the test statistics is a plot of the observed data. This plot is placed randomly in a layout of plots called lineup. The rest of the plots in the lineups are generated from the model specified by the null hypothesis. A human observer is asked to evaluate the lineup. If the observer detects the actual plot in the lineup, the null hypothesis is rejected. 

A lineup is shown in Figure~\ref{fig:lineup_turk} where the observed scatterplot is displayed with a least square line overlaid. Can you find which of these plots show the steepest slope? The rest of the plots or the null plots are generated based on the hypothesis that the slope is zero. If the observed scatterplot is detected that indicates that observed plot is different from null plot and hence the null hypothesis is rejected.

\begin{figure}[htbp] 
   \centering
   \includegraphics[width=6.5in]{plot_turk2_100_450_12_3.png} 
   \caption{A lineup of 20 scatter plots with least square line overlaid. Which of these plots shows the steepest slope?.}
   \label{fig:lineup_turk}
\end{figure}


\cite{majumder:2013}  further developed visual inference by refining the terminologies and providing the methods of obtaining the power. it is revealed that visual test power can be as good as the best available conventional test and in some scenarios even better. This work establishes the validity of lineup protocol to be used as a tool for statistical testing. In the situations where no conventional test exists, visual inference can be the only inferential procedure without compromising a lot in power because of its non-parametric nature and few assumptions. 

These developments open up a whole new area of statistical research where lineups need to be evaluated by human observer. For small scale or day to day research it is possible to have people around to get the lineups evaluated. But sometimes it is needed to have many observers to evaluate lineups. For example, to assess the power of the visual test for different visual test statistics \cite{heike:2012} recruited human observers to evaluate lineups.   \cite{niladri:2012} used lineup protocol in practical applications. Other reasons to evaluate lineups are to present the results of the conventional test with visual tools such as lineup.  



%The effect of different graphical design while selecting visual test statistic is studied by Heike \cite{heike:2012}.

The power of visual test can be obtained theoretically under some assumptions on individual behavior which was evident from experimental studies \citep{majumder:2013}. In general it is hard to obtain explicitly with a mathematical formula since it is very much dependent on individual observation. One approach to estimate the power is to recruit observers to evaluate lineups obtained in known controlled situation. Individual differences in the abilities of correct evaluations of lineup were observed. But Observer's education levels do not seem to have significant effect on performances. Thus it is better to get observers as diverse as possible without being much concerned about the observers' formal training about statistical graphics.

This poses a new challenge to researcher to recruit people from a diverse pool of population. Cost, time, data qualities and convenience are some of the issues that need to be dealt with. Fortunately, we can use the services of Amazon Mechanical Turk web site for this.  

\subsection{Amazon Mechanical Turk (MTurk)}

\cite{turk} Mechanical Turk  or MTurk is an online work place where people from around the world can perform some tasks and get paid. Usually tasks are very simple and no specialized training is required. Being a human is the main requirement. Tasks are designed for anyone to do but some tasks may require that workers satisfy some skill level depending on the recruiters' need. The tasks are designed such that it does not take much time to complete. Humans are still better than computers in performing these types of tasks. The the amount of money paid for each task is very small as well. Figure~\ref{fig:amazon_task} shows an MTurk task where an observer is asked to select from some given options based on a picture. 


\begin{figure}[htbp] 
   \centering
   \includegraphics[width=5in]{amazon_task.pdf} 
   \caption{An example of amazon mechanical turk task. Tasks are usually very simple and designed for human evaluations. With each task, simple instructions are given for workers to follow. The workers first accept the task before submitting their response.}
   \label{fig:amazon_task}
\end{figure}



It is very fast, cheap and reliable to recruit people from MTurk. It allows distributing works to the thousands of workers around the world. Thats why it is getting very popular among the researchers who perform human subject experiment. The another benefit is that a very diverse pool of subjects can be recruited which is otherwise very hard to obtain for a study. The researchers can easily filter the workers based on their experimental design, such as recruiting people only from a specific geographical location or a group of people who satisfy certain criteria etc. The recruiter can decide who they pay or not. Workers have to satisfy the task requirement to ensure payment. But at the end it is the recruiter who has the final say. Usually recruiters pay promptly after the task has been done properly and thats why MTurk is very popular among the online job seeker. 

Because of its convenience it is getting popular for scientific research study as well. In comparison with a lab study \cite{suri:2010} performed the same study using MTurk and demonstrated that their study results are as good as the lab study results even though MTurk study required less time and cost while provided more convenience. \cite{majumder:2013} recruited people from MTurk for their simulation study in estimating the power of visual statistical inference. They have done numerous pilot studies in lab before doing actual MTurk study and found similar results. \cite{mason:2012} explains various features of MTurk and describes how it can be used as part of human behavioral study.

Figure \ref{fig:amazon_task} shows how simple an MTurk task could be. It is possible to get a lineup evaluated by creating such a simple task. We just need to replace the picture in Figure \ref{fig:amazon_task} with the lineup in Figure \ref{fig:lineup_turk} and change the answering options. But some times we may need more than one lineups to be evaluated by an observer. We may need to show a random sample of lineups from a pool of many lineups automatically. The questions the observer would answer while examining the lineup can be different based on different lineup. Moreover, It is convenient if the lineup is clickable so that selection of plot can be made by mouse click. All these are done by creating a web application. The following section discusses this in details.



%Amazon Mechanical Turk web site \cite{turk} is a very good resource for recruiting reliable survey participants for visual experiments. This is the place where people come to work online and get paid. The web site is very convenient and reliable to collect simple experiment data. But to design a complex experiment like what we need using their web interface is a pain and often a very time consuming work. So, to run the turk experiment we felt the necessity for a well customized web site where the lineups presented to the observers could be easily controlled according to the experimental design. The work flow design of such a mechanism is shown in figure \ref{fig:turk_work_flow}. The plan is to forward turk workers to a well customized web site while payment could be processed from turk web site. This facilitates getting data directly in a local server.

%Figure \ref{fig:amazon_task} displays an example of such a task taken from their web site. In this task a worker just has to select from a pool of options and submit the task. The instructions are usually accompany the task and they are very simple and easy to follow. 

\subsection{Getting Turk Workforce}

There are two options to run the web application; one is to run it inside MTurk system using their API and the other is to develop a new web site. For additional control we picked the second option and planned to separate this application from MTurk system and designed our own web site to run the application. It enables to display the lineups to the observers with a lot of flexibilities. As an added benefit the data can be directly saved in a local database server instead of getting it from MTurk. 

Figure \ref{fig:turk_work_flow} shows how the plan works. First an MTurk task is created for the workers to see and decide whether they want to do the task based on the parameters like payment amount, estimated time the task may take to finish, how hard the task seems to be etc. Workers are informed that the task has to be done outside the MTurk system from another web site. Once the workers accept the task they are redirected to our web site where multiple lineups are shown for evaluations. After the required number of evaluations has been received, a code is generated which the workers has to submit back in MTurk system to complete the task. The code is matched with the code in our database to process payment through MTurk system.

\begin{figure}[htbp]
   \centering
       \scalebox{.4}{\includegraphics{turk_work_flow.pdf}}
       \caption{Amazon Mechanical Turk workflow shows how data are collected through turk experiment web and payment is processed through MTurk system.}
       \label{fig:turk_work_flow}
\end{figure}

MTurk will provide workers who are used to doing simple task. Thus, for lineups to be evaluated a simple task needs to be created through MTurk. But People who need to make statistical decision based on lineup will need a system that can provide an easy solution to this challenge.  This paper provides a complete solution to this. This paper is organized the way a researcher may work with lineup and like to get feedback data from the observer. Section \ref{sec:turk_exp} presents the details description of an experiment with lineups and things to consider before a turk task. Section \ref{sec:web_application} presents the design of an web application to get lineups evaluated with additional flexibilities and options that may not be possible through MTurk. Section \ref{sec:turk_task} describes how to create an MTurk task, mange workers and process payments. Finally Section \ref{sec:turk_data} presents some data obtained from various experiments done through the web application and discusses about the quality of the data.


\section{Experiment Design} \label{sec:turk_exp}

If a lineup is created from actual observed data, the web application presented in Section \ref{sec:web_application} can be used to evaluate it. In that case we do not need to design a simulation experiment. This section will be useful when a simulation experiment is needed to examine different plot types as test statistics and compare the power of visual test \citep{majumder:2013}. To examine the effectiveness of certain plots in displaying the data one might want to design an experiment with lineup \citep{heike:2012}. In any cases, two major considerations are choice of parameters to simulate lineup and number of people needed to evaluate a lineup.

\subsection{Selecting Parameter to Simulate Lineup} \label{sec:parameter_selection} In any experimental design we need to have some control. For a simulation experiment with lineup, this refers to the parameter related to a lineup. This depends on what the purpose of the experiment is. For example, to compare the power of a parametric test one may fix the parameter specified by alternative hypothesis and generate data from that model. This data can be considered as the observed data. A lineup can be created by placing this observed data plot on a lineup where rest of the data plots come from the model specified by the null model \citep{majumder:2013}. 

In addition to the hypothesized parameter of interest, other parameters that may need to be fixed include sample sizes, variability in the data, error structure etc. These parameters produces the effect sizes and are responsible for any detectable signal in the lineup. Thus for a simulation study a range of effects sizes need to be considered. It is a little bit tricky to decide what exact effect sizes are appropriate. For example in \cite{majumder:2013} the parameters were chosen so that they produce a smoother power curve of a conventional test. But in general this depends on what the researchers may want to test and control in an experimental study.

%The very first consideration is the sample size. Other parameters depends on the model of interest. For example we have fixed the parameters in our first Amazon Mechanical Turk experiment as shown in table \ref{tbl:experiment_params}.

\subsection{Procedure to Simulate Data Plot} \label{sec:simulate_plot} In the simulation study the very first step is to generate a random sample of data from model of interest for pre-specified parameters. We call this data set observed data. Every time we do that we get a new set of observed data which may produce a parameter very different than the actual parameter specified to generate the data. But it is important to have a observed data closely representing the parameters chosen since other null plots will be compared to this data plot in a lineup. One solution to this is to take some replications of lineups for the same effect size. 

While the effect of the natural variability of the observed data set can be controlled by taking the replication of few samples, it is desirable to study whether we can reduce this variability further to some extent. For this we pick the example of a simple linear regression model and examines how the data plot can be generated that best represent the specified parameter.  Consider a linear regression model 
\begin{equation} \label{eqn:turk_model} 
Y_i = \beta_0 + \beta X  + \epsilon_i 
\end{equation}
where $\epsilon_i \stackrel{iid}{ \sim } N(0,\sigma^2)$, $i=1,2, .., n$. The covariate $X$ can be continuous or discrete. As discussed in Section \ref{sec:parameter_selection} we specify the parameters sample size $n$=300, regression slope $\beta$=3 and error standard deviation $\sigma$=12 to simulate observed data.

To make sure that the observed data set truly represents the Model \eqref{eqn:turk_model} we suggest three approaches. One is Kolomogorov test statistic approach and the other two approaches are quantiles of $p$-values and closeness of estimated parameters to the true parameters. To illustrate the three approaches we used Model \eqref{eqn:turk_model} as an example but the idea can be extended to any situation. The three approaches are discussed below. \\

{\bf Kolomogorov test statistic approach:} In this approach we simulate 1000 data sets from Model \eqref{eqn:turk_model}  and obtain Kolmogorov test statistic for each set of data as below. $$D_n=\sup_x |F_n(x)-F(x)|$$ where $F_n(x)=\frac1n \sum_{i=1}^n I_{X_i\le x}$ be the empirical distribution function of fitted residuals, $I_{X_i\le x}$ be the indicator function equal to 1 if $X_i\le x$ and equal to 0 otherwise and $F(x)$ be the cumulative function of normal with mean zero(0) and variance $\sigma^2$. We keep the data set that has minimum value for Kolmogorov test statistic since for this data set $F_n(x)$ should be the closest to the desired normal model.  \\

{\bf Quantiles of $p$-value approach:} For a simulated observed data set we can obtain the $p$-value associated to testing $H_0: \beta=0$. The distribution of $p$-value is uniform under null hypothesis but under alternative it has a right skewed distribution. Figure~\ref{fig:dist_pvalue} shows the distribution  of $p$-values for sample size $n$=300, regression slope $\beta$=3 and error standard deviation $\sigma$=12. If we generate a data set randomly there is a 21\% chance that the data will show a $p$-value of 0.25 or more even though we generated data set with non-zero $\beta=3$. We need to make sure that the simulated data does not come from this extreme end. On top of that we want some replication so that this extreme effect can be controlled at some level. For this example we intend to pick three replications.


\begin{figure}[hbtp]
   \centering
       \scalebox{.6}{\includegraphics{dist_pvalue.pdf}}
       \caption{Distribution of $p$-values for $n$=300, $\beta$=3 and $\sigma$=12.}
       \label{fig:dist_pvalue}
\end{figure}

In this approach we generate 1000 data sets from the model of interest and obtain $p$-values for each data set after fitting the same model. We construct 3 blocks of $p$-values such as (0.0-$q_{33}$), ($q_{33}$-$q_{66}$), ($q_{66}$-1) where $q_i$ is the $i$th percentile in the distribution of the $p$-values. We randomly select three data sets that have corresponding $p$-values in the above quantile range.  Notice that if we like to have three replications of plots, we will have observed data sets with smaller $p$-values as the distribution is highly right skewed as well as the 33th and 66th percentiles of p-values are 0.0101 and 0.077 respectively. So, another option may be to take the mid points of those quantile ranges and pick the data sets that have those p-values. \\

{\bf Closeness of estimated parameters:} When we simulate a data set from the Model \eqref{eqn:turk_model} and fit the model to the data set again, we do not get the parameters estimates same as what we fixed while simulation. For example, we obtained a parameter estimates of 0.5 for true $\beta=3$. We want a data set where the parameter is closed to our fixed slope $\beta=3$.  In this approach we generate 1000 data sets and pick the data set that shows most close estimates of parameters compared to true parameter values.

All of these three approaches discussed above has some problems. Kolmogorov method does not necessarily make sure that estimated parameters are close to the true parameters. On the other hand closeness of estimated parameters does not make sure that $p$-value is small enough when we should reject the null hypothesis. Only the quantiles approach seems reasonable as it has much control over $p$-values and it gives similar data sets that has closest parameter estimates. So we suggest the quantiles of $p$-value approach to generate observed data with specific effect size.

\subsection{Sample size estimation} By sample size we mean the number of evaluations per lineup. It is different than number of people who evaluates lineup. If multiple lineups are evaluated by a person, sample size will be larger than number of people recruited from MTurk. 

To have an idea how many evaluations are required for a lineup one approach can be to have a prior idea of proportion correct for the lineup. For a given proportion $p$ we want to have margin of error (ME) to be 0.1. Thus we have $$ME =1.96 \sqrt{ \frac 1 n p(1-p)} \le 0.05$$ which gives us the estimation of minimum sample size $$n \geq \frac{p(1-p)}{(0.05/1.96)^2}.$$ 

If we do not have any prior idea of proportion of correct or power, we need to rely on the power of similar conventional test. This can be assumed to be the power or proportion correct for that lineup. Other ways of having an estimate of sample size can be very specific to the problem of interest and should be dealt with the requirement of the problem.

\subsection{Test and Training Lineup} Some lineups should be presented to the MTurk worker so that they can become familiar with the experimental environment before they participate the actual experiment. It is important to setup this properly. The training lineups should be easy and feedback should be provided right after the feedback received. The purpose is to give them an idea how the actual task would look like.

The another way can be to make this training mandatory to participate the experiment. The MTurk worker should have certain proportion of correct responses on these training lineups to allow them to participate. This is often practiced in MTurk task and the workers are very familiar with this approach. In MTurk language it is called the qualification of the worker. Thus to design a MTurk task it is important to set an standard qualification of the worker.

Other things that needs to be considered include number of training lineups, how the lineups should be shown. Training should not take much longer. Two or three lineups can provide enough training. The training lineups should be easy to evaluate and they should be randomly selected from a very small number of lineups. That may produce repetition which may help the MTurk workers to retest their skill on the lineup the might have a wrong selection before. 

There should be a test lineup when the workers evaluate multiple lineups in actual experiment. This test lineup can be used to process payment and detect any unusual responses. The test lineup should be super easy to evaluate so that anyone can detect the data plot without much effort.

It is helpful if some example lineups are presented with the experimental description. Example lineup on the homepage should be of size three or four so that more than one lineup can be placed on the homepage. This helps the worker decide whether they are willing to participate the experiment at all.

\subsection{Plan for a Turk Task} \label{sec:task_plan} To make a good use of MTurk crowd source, it is important to split any big project into small pieces or Task. To present each task for the workers to do, a Human Intelligence Task or HIT needs to be designed which includes crisp and clear instructions of the task, descriptions of specific input and output desired and payment information. To design a HIT for lineups to be evaluated we need to consider the following issues;

\begin{itemize}

\item {\bf IRB approval:} Since this is a human subject experiment proper approval has to be taken from Institutional Review Board or IRB. This approval includes how the task will be performed, how the anonymity of the subjects will be maintained etc. For this we plan to have a consent form which the MTurk workers have to agree before participating the experiment. The consent form is approved by IRB.
 
\item {\bf Lineup question:} Each lineup should be presented with a question that the worker has to answer. The question needs to be clear and technical word should be removed as the MTurk workers may not be aware of those terms. The question is important for lineups as well. Very common question to evaluate a lineup is ``Which plot is the most different?". But for complicated studies the question may be quite different \citep{majumder:socio}.

\item {\bf Data input:} For lineup experiments data input means mainly the lineups that are going to be presented for evaluations. Lineups may be presented in some sequence or completely at random. We need to decide whether a test lineup will be included or not in the pool of lineups going to be presented. Some other inputs may accompany with each lineup. Those include lineup question, how many lineups to be evaluated by the worker etc. Anything that the worker will see changing with each lineup is considered as data input to the worker. For different experimental needs this input may be different. For example, one may decide to let the worker know if each evaluation made is correct or wrong right after the feedback. This is an input for the worker.

\item {\bf Data output:} This means the data collected from the MTurk workers. That includes mainly the response on lineup question. Single or multiple responses can be requested for each lineup. Other optional data that may be collected are as follows;

\begin{enumerate}
\item Demographic information of the observer
\item Supplementary information such as reasons or confidence levels of the response provided
\item Location information
\item Time of the evaluation
\end{enumerate}

\item {\bf Payment:} Payment plays an important role on data quality and how fast the data can be collected. It depends on how many lineups are being evaluated by a worker, how much time it may require to finish the task or how hard the task is in general.

%Whether reasoning would be collected or not
%
%Multiple or single response
%
%Test plot added or not
%
%Training needed or not
%
%Feedback given after every evaluation or not
%
%How many lineups to show
%
%Are the lineups randomly selected?

\end{itemize}


\section{Web Application for Turk Experiment} \label{sec:web_application}

A web site is developed to get lineups evaluated by human observers \citep{majumder:turk}. It provides all the features needed for a simulation experiment with lineups but yet remains simple for the online workers. This section describes the technical details of the site. 

The web application is built using server side scripting language PHP embedded in HTML. JavaScript is used to control the client side work flow such as preventing missing information, showing messages etc. MySQL database is used to store data for dynamic presentation of lineup and recording observer feedbacks. The application also records the ip address of the observer's machine and the time of each display of lineup and its evaluation.  

%We developed a web site to run the Amazon Mechanical Turk experiments.  PHP scripting language is used and embedded with HTML for designing the web site. Javascript is used for validating the input data and producing warning message.  We use  Amazon Mechanical Turk web site to recruit the participants and direct them to our web site to actually participate in the survey. This has allowed us to get the data directly in a secure way from the web and we don't have to depend on the Mechanical Turk web site to transfer data to our server. 

\subsection{Form Design}

Designing a data collection form is very critical in turk experiments. As we see from Figure \ref{fig:amazon_task} that a turk task needs to be as simple as possible. This is what the turk workers are prepared for. Making a complex form may turn out bad and jeopardize the whole purpose of the experiment. 

Keeping these in mind, two web forms are designed to collect information from the turk workers. The first form shown in Figure \ref{fig:turk_web} collects feedback information about a single lineup. The information collected through this form is all about the lineup that includes plot number selected, reasons for selection and the confidence level of the selection. Each turk user is identified by the nick name which is mainly the turk ID. It looks simple and it is indeed simple for the turk worker to provide feedback using this form. But it is not simple in design as all the information on this form are coming from database including the question on top of the lineup. This dynamic page provides a lot of features in customizing how one may want to display the lineup.

\begin{figure}[hbtp]
   \centering
       \scalebox{.4}{\includegraphics{turk_web.pdf}}
       \caption{A sample data collection form. Lineups are presented at random for evaluations by the turk workers. Scalable Vector Graphics (SVG) is used so that observer can click on the lineup to pick certain plot. Once a plot is selected it gets shaded and the number appears in the choice text box.}
       \label{fig:turk_web}
\end{figure}

One of the nice features of the form in Figure \ref{fig:turk_web} is the use of Scalable Vector Graphics (SVG) for lineup which enables an observer to give feedback with the ease of just a mouse click. If the observer changes mind he or she can click the plot again and deselect it. If needed multiple selection of plots can be allowed, order of selection and deselection can be recorded with time taken for each action. Thats what we meant by saying it is complex in design. 

It is possible to let workers see the statistics of their total feedbacks with number of correct evaluations. This feature can be opted out easily if not necessary. The worker has to provide their turk ID and for next evaluation they don't have to type it again. This allows the worker give feedback using only mouse click. Thats what we meant by saying it is simple for turk worker.

Once the data is submitted a feedback can be provided whether their choice was correct or wrong. This feature can also be opted out easily if not needed. The design of the feedback form is shown in Figure \ref{fig:turk_web_feedback}. This form is also used to collect demographic and educational information about the worker. After the required number of evaluations are obtained, a pass code is given as a proof of the completion of the task which is used for payment purpose later.

\begin{figure}[hbtp]
   \centering
       \scalebox{.4}{\includegraphics{turk_web_feedback.png}}
       \caption{The turk workers are given feedbacks whether their evaluation for each lineup was correct or not. This works as an incentive for the worker to work more enthusiastically. To ensure the payment, the turk workers have to provide some information using this form.}
       \label{fig:turk_web_feedback}
\end{figure}

Each worker is shown some specific number of lineups for evaluation. These lineups are randomly selected from a pool of lineups designed for evaluations. The algorithm for how the lineups should be selected to show and what would be the order of display is implemented in the form shown in Figure \ref{fig:turk_web}. Also there is a check for invalid or missing information which is implemented using javascript. If users try to go forward without giving any feedback they are not allowed to do that showing a warning message.

%After the evaluation is recorded on a lineup through form shown in Figure \ref{fig:turk_web} another form shown in Figure \ref{fig:turk_web_feedback} is used to give feedback to the turk users whether they correctly identifies the true plot and how many evaluations are recorded from the users. 


\subsection{Database design} The design of the database to store the collected data is shown in Figure \ref{fig:turk_database_design}. It is designed such a way that data from many different experiments can be stored in the same database. In total five separate tables are used. Table turk\_worker contains static information about each turk worker. The location information of each turk worker is stored in ip\_detals table. The information in this table is collected later based on the ip address of each turk worker. Table picture\_delais contains the static information about each lineup. Table feedback is a dynamic table that grows with the number of feedbacks from each turk worker. The multiple activities of the worker are recorded in turk\_activity table. This table also contains the codes provided to the workers once they finish the experiment. For the implementation of the web application we used MySQL database located on a local server.

\begin{figure}[hbtp]
   \centering
       \scalebox{.4}{\includegraphics{turk_database_design.pdf}}
       \caption{Relational database design for MTurk experiment data collection. The same database can be used for multiple turk experiments by keeping experiment information in picture\_details table which contains information about the lineups.}
       \label{fig:turk_database_design}
\end{figure}

\subsection{Data collection} 

The homepage of the web displays detailed explanation on how they can perform the task of evaluating the lineups. Several examples with possible answers are provided. It is possible to customize how the workers will proceed from the home page. There are two options; one is to allow them to continue the task where no trial is needed. The second option is to force them to try some lineup before joining the actual experiment. The trial feedbacks are not recorded. As per the requirement of Institutional Review Board (IRB) the workers need to provide the informed consent. For this they have to read and agree with specific IRB approved informed consent. The flow chart for this data collection sequence is shown in Figure \ref{fig:turk_data_flow}.

\begin{figure}[hbtp]
   \centering
       \scalebox{.4}{\includegraphics{turk_data_flow.png}}
       \caption{Data collection work flow shows that workers can try some test lineups before going to the live experiment after providing informed consent. This design gives the flexibility to make the trial mandatory, if needed, so that without having enough correct trial evaluations the actual participation can be prevented.}
       \label{fig:turk_data_flow}
\end{figure}


%To collect the survey data we developed a web site that shows a plot to an individual and get feedback from that individual. Each individual is asked to provide feedback for at least 10 different plots. The 10 plots that are shown to an individual are randomly chosen with probability proportional to the number of responses required (the sample size) for that plot.

%The MTurk workers were redirected to the web site and the data were collected, stored automatically into a local database server. Demographic informations such as age group, gender and education levels were also collected. The time taken for each evaluation is computed based on the time the plot was shown and the time the feedback was received. The location of the observer is determined by the ip address of the observer.

%Figure \ref{fig:turk_work_flow} turk workers have to visit the experimental web site and give their consent to participate the survey as per IRB requirement. In the experimental web, there is a option for observers to try some sample lineups to get familiar with the experiment. Once the observers start evaluating the lineups they are sequentially shown a specific number of lineups and at the end they are given a code as a proof for their tusk completion. The payment of the turk worker is processed if they post the code they got from experimental web site. The observers are also asked to provide some demographic information about them to make sure that the code is generated.

The default information that the system is designed to collect from each individual is shown in Table \ref{tbl:data_info}. Data received from each individual will be automatically saved in a secured mysql server. 

\begin{table}[hbtp]
\caption{Information Collected from Each Individual}
\centering 
\begin{tabular}{lp{8cm}} 
\hline
Information &  Description \\ %[0.5ex] % inserts table %heading 
\hline
Identification & Nick name or any ID to determine the responses of an Individual \\
Response number & The number of the plot on the lineup plot which the individual thinks the most different than other 19 plots.\\ 
Reason of of choice & Reason why the individual chooses the plot \\
Confidence level & Confidence level of individual choice \\ 
Age group& The age group where the individual belongs \\
Education & The highest level of education completed \\
Gender & Male or female \\
Geographic Location & This information is collected through the ip address of the individual computer \\ 
Time taken & Time taken for each response\\
\hline
\end{tabular}
\label{tbl:data_info}
\end{table}	


%\subsection{Data collection sequence} 


\subsection{Data Security and Validity} 

Some cautious attempts are taken to add security to the data. Server side scripting language PHP is used to connect to database and access or save data. For transferring data from one form to another PHP session variables are used. Cookies are avoided carefully so that no important informations are saved in the cookies. 

To prevent missing information and invalid input JavaScripts are used. For controlling some flow of the work such as showing various messages, JavaScripts are used as well. Careful consideration are made so that no important informations are stored in the java variable or function that could easily be revealed.


\section{Managing Turk Task}\label{sec:turk_task}  

Turk workers are usually small payee workers. There are several issues related to the payment of their effort. These are briefly discussed in the following section. 

% and we intend to work more on this as we progress through this dissertation work. 

\subsection{Creating Task for Lineup Evaluation} Task description, number of people to recruit, time allowed to finish the task, specify the qualification needed, setting the duration of the task, offering the pay per task. The task needs to be created as per the task plan described in Section \ref{sec:task_plan}.

\subsection{Accepting or Rejecting the Task} As per design we keep a test lineup plot which is very easy to evaluate. The general rule may be to reject payment when workers do not get that easy test lineup plot. Our experience suggests that there are workers who do not get that test plot but their overall success rate is very high. So depending on only one criteria the payment should not be made. 

%These issues would be addressed in our future work based on the several turk experiments that we intend to carry out.

\subsection{Payment for the Worker}Amazon Mechanical Turk suggests that each worker gets \$6/hour as wage. Thus we need to estimate how much time a task may take to complete and based on that we have to specify the payment for each task. Our experience indicates that a better payment brings more serious workers who usually provide cleaner data which is more reliable. 

%We may study this issue while we do multiple turk experiments.

\subsection{Managing Worker} Pay bonus, appreciate worker by sending email, block worker etc.

\section{Turk Data Quality} \label{sec:turk_data}

In this section we intend to analyze the performance of the experiments we did using Amazon Mechanical turk. We have feed back data for the similar plots from both turk workers as well as from the people who regularly participates in the graphics group meeting of our department. We consider later participants as more knowledgeable and trained in seeing pattern in statistical plots. We can use these data to evaluate the performance of the general participants from around the world.



\begin{table}[hbtp]
\caption{Amazon mechanical turk experiments and their properties. Duration in hours per 100 tasks show the popularity of some tasks compared to others.}
\centering
\scalebox{0.9}{
\begin{tabular}{rlrrrrrrr}
  \hline
& Experiment& \multicolumn{2}{c}{ Total Task}& Average & \multicolumn{2}{c}{Duration (hour)} & Payment & Pay rate\\
\cline{3-4} \cline{6-7}
Serial & description & submitted & rejected & time(min) & Actual & 100 task& \$/task & \$/hour\\ 
  \hline
1 & Boxplot & 406 & 106 & 10.68 & 146.48 & 36.08 & 0.50 & 2.81 \\ 
  2 & Scatterplot & 359 &   9 & 10.80 & 42.68 & 11.89 & 1.00 & 5.58 \\ 
  3 & Contaminated plot & 219 &  19 & 13.53 & 126.17 & 57.61 & 1.00 & 2.22 \\ 
  4 & Polar vs Cartesian & 110 &  10 & 20.65 & 11.65 & 10.59 & 1.00 & 2.91 \\ 
  5 & Hist vs density & 234 &  37 & 17.85 & 41.57 & 17.76 & 1.00 & 3.36 \\ 
  6 & Violin vs boxplot & 417 &  17 & 17.95 & 105.87 & 25.39 & 1.00 & 3.34 \\ 
  7 & Group separation & 106 &   6 & 16.13 & 5.15 & 4.86 & 1.00 & 3.72 \\ 
  8 & Sine Illusion & 101 &   1 & 16.52 & 78.38 & 77.60 & 1.00 & 3.63 \\ 
  9 & Gene expression & 103 &   3 & 12.47 & 11.27 & 10.94 & 0.50 & 2.41 \\ 
  10 & Test normality & 406 &   6 & 22.70 & 74.35 & 18.31 & 1.00 & 2.64 \\ 
   \hline
\end{tabular}
}
\label{tbl:mturk}
\end{table}


\begin{figure}[htbp] 
   \centering
   \includegraphics[width=3in]{rejected_task.pdf}
      \includegraphics[width=3in]{task_duration.pdf} 
   \caption{Percentage of rejected tasks and duration of each experiment in hour per 100 tasks for each of the 10 experiments. Most of the tasks got rejected for box plot experiment.  Even though the sine illusion experiment took longest to finish the rejection rate is lowest for this experiment.}
   \label{fig:task_duration}
\end{figure}


\subsection{Data Cleaning} Some suggestion about data cleaning based on experimental evidence. What to expect from turk data.

\subsection{Diversity in the Data} Demographics, geographical locations, academic diversity

%\begin{figure}[hbtp]
%   \centering
%       \scalebox{.5}{\includegraphics{turker_country.pdf}}
%       \scalebox{.5}{\includegraphics{turker_location.pdf}}       
%       \caption{The location of turk workers on the world map shows the geographical diversity of turk workers even though most of the workers come from India and United States.}
%       \label{fig:rsq}
%\end{figure}

\subsection{Selection Bias} 

%Since data are collected through web interface it is possible to have the selection bias in the turk experiment we did. In this section we intend to analyze to what extent this selection bias has occurred and how we can adjust for this.


Time of the task may recruit from a specific geographical location

Setting up qualification may allow certain workers to do the task

Payment amount may affect the duration of the experiment which may avoid diversity in the geographical locations.


\section{Conclusion} This paper presents a complete solution to the problem of designing a simulation experiment for lineup and recruiting people to get the lineups evaluated. The online application design provides features to control how the multiple lineups can be presented to an observer. Scalable Vector Graphics (SVG) are used for lineup so that observer can click on the lineup to pick certain plot. This also made the multiple plot selection from a lineup easier and convenient. Multiple online experiments are done using this web application and result suggests that the application worked as planned.

One of the main features of the web application is that it produces simple task for the worker but still retains many flexibilities to the researcher who need lineups to be evaluated as per complex experimental requirement. The design of this application allows recruiting people from any source not necessarily just from MTurk. The web application is now hosted on Iowa State University public domain \citep{majumder:turk} and any number of experiments can be done through this web site without changing the core of this application.

The next direction of this work is to make a complete package so that anyone can reproduce this web application, customize according to their need and run the MTurk experiments as part of their research that involves lineups. This will also help lineup protocol to be used frequently since it will provide an ease of its use in making inferential decisions.

We also intend to set up a web application for public use where researcher can put their lineup for online evaluation. The observer can be recruited from MTurk or any other source including local lab participants. 


%
%\section{Appendix: What do Turk Workers Pick, $R^2$ or $p$-value?} What do people pick in a lineup plot, p-value or $R^2$? To address this issue we obtain the approximate theoretical relationship between $R^2$ and the power of the classical hypothesis test. Suppose we want to test the significance of the slope ( i.e.,  $H_0: \beta=0$ against $\beta \ne 0$) of a continuous covariate $X$ in  a simple linear regression model setting. Also consider $X \sim N(0,1)$. For a sample of size $n$ this gives us 
%\begin{eqnarray*}
%Y_i-\bar{Y}& = & \beta_0+\beta_1X_i+\epsilon_i - \beta_0 - \beta_1 \bar{X}- \bar{\epsilon} \\
%          & = & \beta_1(X_i-\bar{X})+(\epsilon_i-\bar{\epsilon})
%\end{eqnarray*}
%Now 
%\begin{eqnarray*}
%X_i-\bar{X}& = & X_i - \frac1n (X_1 + X_2 + ... + X_i + .....+ X_n) \\
%          & = & X_i - \frac1n X_i - \frac1n \sum_{j \neq i}{X_j}\\
%          & = & (1-\frac1n)X_i - \frac1n \sum_{j \neq i}{X_j}
%\end{eqnarray*}
%
%Thus we have 
%\begin{eqnarray*}
%E(X_i-\bar{X})^2 & = & (1-\frac1n)EX_i^2 - \left( \frac1n \right )^2 (n-1) EX_j^2 \\
%                 & = & (1-\frac1n)^2+ \frac{n-1}{n^2}\\
%                 & = & \frac{n-1}{n}
%\end{eqnarray*}
%
%Similarly we have 
%
%\begin{eqnarray*}
%E(\epsilon_i-\bar{\epsilon})^2 & = & (1-\frac1n)E\epsilon_i^2 - \left( \frac1n \right )^2 (n-1) E\epsilon_j^2 \\
%                 & = & \left((1-\frac1n)^2+ \frac{n-1}{n^2} \right) \sigma^2 \\
%                 & = & \frac{n-1}{n}\sigma^2
%\end{eqnarray*}
%Finally we have expected total sum of square (SST) as
%\begin{eqnarray*}
%E\sum_i{(Y_i-\bar{Y})^2} & = & \sum_i E(Y_i-\bar{Y})^2=\sum_i \left [ \beta_1^2 E(X_i-\bar{X})^2 + E(\epsilon_i-\bar{\epsilon})^2 \right] \\
%                 & = & \sum_i\left[ \beta_1^2 \frac{n-1}{n}+  \sigma^2 \frac{n-1}{n}\right]\\
%                 & = & (n-1)(\sigma^2+\beta_1^2)
%\end{eqnarray*}
%
%We know $E(MSE|X) = \sigma^2$ or $E(SSE/n-2)|X = \sigma^2$ which gives the expected residual or error sum of square as $$E \sum_i (Y_i-\hat Y_i)^2=(n-2)\sigma^2$$
%Thus we have the expected regression sum of squares $SSR =  (n-1)(\sigma^2+\beta_1^2) - (n-2)\sigma^2 = (n-1) \beta_1^2+\sigma^2$ This gives $$R^2=\frac{(n-1) \beta_1^2+\sigma^2}{ (n-1)(\sigma^2+\beta_1^2)}$$
%
%
%\begin{figure}[hbtp]
%   \centering
%       \scalebox{.3}{\includegraphics{scatter_plot_beta_4.pdf}}
%       \caption{Scatter plot of data generated for $\beta_0$=6, $\beta_1$=4, sample size $n =300$ and $\sigma = 12$. $R^2$ for this data was obtained as 0.1309 showing that the model could not successfully explain the variation in $Y$. But notice in figure \ref{fig:power_rsq} that power for $\beta$=4 is almost 1. So, Can we conclude that $R^2$ does not have any effect in deciding whether $H_0: \beta_1=0$ should be rejected or not?}
%       \label{fig:plot_beta_4}
%\end{figure}
%
%
%\begin{figure}[hbtp]
%   \centering
%       \scalebox{.35}{\includegraphics{power_300_12.pdf}}
%       \caption{Power curve and $R^2$ values for sample size $n =300$ and $\sigma = 12$. Notice that for a small value of $R^2$ (0.1) the power is almost 1.}
%       \label{fig:power_rsq}
%\end{figure}
%
%
%\begin{figure}[hbtp]
%   \centering
%       \scalebox{.35}{\includegraphics{rsquare_contour.pdf}}
%       \scalebox{.35}{\includegraphics{rsquare_beta_sigma.pdf}}
%       \caption{Contour and surface plots of $R^2$ for sample size $n =300$. The values for $R^2$ goes down sharply with $\sigma$ and $\beta$.}
%       \label{fig:contour_rsq}
%\end{figure}
%
%\begin{figure}[hbtp]
%   \centering
%       \scalebox{.35}{\includegraphics{rsquare_beta_sigma_lines.pdf}}
%       \scalebox{.35}{\includegraphics{rsquare_beta_sigma_lines1.pdf}}       
%       \caption{Relationship of $R^2$ with $\sigma$ and $\beta$ for sample size $n =300$. The values for $R^2$ goes down sharply with $\sigma$ and $\beta$.}
%       \label{fig:rsq}
%\end{figure}
%



%\bibliographystyle{plain}
\bibliographystyle{asa}
\bibliography{references} 


\end{document}


